{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"pull-right\"><img src=KEY-logo.png></div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "### Exploring the NLP pipeline in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSI4106 Artificial Intelligence  \n",
    "Fall 2018  \n",
    "Caroline Barrière\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is split in two parts.  In **Part A**, you will explore the different steps of the NLP pipeline, and in **Part B**, you will perform a few statistical analysis of a corpus.  We will work with the package *nltk* which is very useful for NLP analysis.  You will need to install it before you start.  Information about NLTK are here: http://www.nltk.org/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***HOMEWORK***:  \n",
    "Go through the notebook by running each cell, one at a time. Look for (**TO DO**) for the tasks that you need to perform.  \n",
    "Make sure you *sign* (type your name) the notebook at the end. Once you're done, submit your notebook.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first step, import the package\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART A - NATURAL LANGUAGE PROCESSING PIPELINE**  \n",
    "  \n",
    "In this part, we will use the modules from *nltk* to perform the different steps of the pipeline.  \n",
    "We first define a small sample text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleText = \"On September 5th, we started our A.I. course. The course number is CSI4106. \"\\\n",
    "             \"In that course, we have studied many artificial intelligence concepts.  \"\\\n",
    "             \"We have looked at intricate algorithms, such as the ARC-3 algorithm and neural networks learning algorithms.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(sampleText)\n",
    "# number of tokens\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'September', '5th', ',', 'we', 'started', 'our', 'A.I', '.', 'course', '.', 'The', 'course', 'number', 'is', 'CSI4106', '.', 'In', 'that', 'course', ',', 'we', 'have', 'studied', 'many', 'artificial', 'intelligence', 'concepts', '.', 'We', 'have', 'looked', 'at', 'intricate', 'algorithms', ',', 'such', 'as', 'the', 'ARC-3', 'algorithm', 'and', 'neural', 'networks', 'learning', 'algorithms', '.']\n"
     ]
    }
   ],
   "source": [
    "# Showing the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2a - Stemming (Porter Stemmer)\n",
    "For a reference to the [algorithm](http://snowballstem.org/algorithms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'septemb', '5th', ',', 'we', 'start', 'our', 'a.i', '.', 'cours', '.', 'the', 'cours', 'number', 'is', 'csi4106', '.', 'In', 'that', 'cours', ',', 'we', 'have', 'studi', 'mani', 'artifici', 'intellig', 'concept', '.', 'We', 'have', 'look', 'at', 'intric', 'algorithm', ',', 'such', 'as', 'the', 'arc-3', 'algorithm', 'and', 'neural', 'network', 'learn', 'algorithm', '.']\n"
     ]
    }
   ],
   "source": [
    "# nltk contains different stemmers, and we try the Porter Stemmer here\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "singles = [stemmer.stem(t) for t in tokens]\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2b - Lemmatization\n",
    "The lemmatization relies on a resource called Wordnet (https://wordnet.princeton.edu/), in which lemmas are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download the wordnet resource\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'September', '5th', ',', 'we', 'started', 'our', 'A.I', '.', 'course', '.', 'The', 'course', 'number', 'is', 'CSI4106', '.', 'In', 'that', 'course', ',', 'we', 'have', 'studied', 'many', 'artificial', 'intelligence', 'concept', '.', 'We', 'have', 'looked', 'at', 'intricate', 'algorithm', ',', 'such', 'a', 'the', 'ARC-3', 'algorithm', 'and', 'neural', 'network', 'learning', 'algorithm', '.']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmas = [wnl.lemmatize(t) for t in tokens]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO : Q1)** Describe in your own words the difference between lemmatisation and stemming.  Use examples from above to show the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q1 ANSWER:* Stemming is the creations of a set of non-chaning portion of words, while lemmatisation creates a set of the base form of words, or lemmas. In the above code the words 'September' and 'course' were changed to 'septemb' and 'cours' in the stemming, but in the lemmatisation they became 'september' and 'course'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Part-Of-Speech tagging  (POS tagging)\n",
    "As we've seen in class, sentence splitting can be learned through a supervised model.  POS tagging can also be learned through a supervised model.  Here, we will use a perceptron model pre-trained in NLTK.  Look here http://www.nltk.org/_modules/nltk/tag/perceptron.html to understand the model.  \n",
    "  \n",
    "The full sest of tags is available [here](https://www.clips.uantwerpen.be/pages/mbsp-tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[('On', 'IN'), ('September', 'NNP'), ('5th', 'CD'), (',', ','), ('we', 'PRP'), ('started', 'VBD'), ('our', 'PRP$'), ('A.I', 'NNP'), ('.', '.'), ('course', 'NN'), ('.', '.'), ('The', 'DT'), ('course', 'NN'), ('number', 'NN'), ('is', 'VBZ'), ('CSI4106', 'NNP'), ('.', '.'), ('In', 'IN'), ('that', 'DT'), ('course', 'NN'), (',', ','), ('we', 'PRP'), ('have', 'VBP'), ('studied', 'VBN'), ('many', 'JJ'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('concepts', 'NNS'), ('.', '.'), ('We', 'PRP'), ('have', 'VBP'), ('looked', 'VBN'), ('at', 'IN'), ('intricate', 'JJ'), ('algorithms', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('the', 'DT'), ('ARC-3', 'NNP'), ('algorithm', 'NN'), ('and', 'CC'), ('neural', 'JJ'), ('networks', 'NNS'), ('learning', 'VBG'), ('algorithms', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# nltk contains a method to obtain the part-of-speech of each token\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "posTokens = nltk.pos_tag(tokens)\n",
    "print(posTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('September', 'NNP')\n",
      "NNP\n"
     ]
    }
   ],
   "source": [
    "# If we just want to see one tag in particular\n",
    "\n",
    "print(posTokens[1])  # it's a tuple\n",
    "print(posTokens[1][1])  # second part of the tuple is the tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to Step 2 \n",
    "\n",
    "The lemmatizer we use is based on WordNet (a lexical resource commonly used in NLP) to provide a set of lemmas.  As many words are ambiguous and can be found in sentences as verbs or nouns (remember examples such as *Will's will will be achieved*), the lemmatizer can benefit from knowledge of POS.  Small problem... POS tags in Wordnet are not the same as in Treebank.  Wordnet defines only 4 POS: N (noun), V (verb), J (adjective) and R (adverb). The small method below is to obtain a partial equivalence between the tagsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# try to lemmatize, this time knowing the POS\n",
    "# tagsets are often different... here we map the treebank tagset (default in pos_tag) \n",
    "# to the wordnet tagset -- \n",
    "# We will learn more about wordnet when we discuss resources in the Knowledge Representation module of this course\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.ADV  # just use as default, for ADV the lemmatizer doesn't change anything \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r', 'n', 'r', 'r', 'r', 'v', 'r', 'n', 'r', 'n', 'r', 'r', 'n', 'n', 'v', 'n', 'r', 'r', 'r', 'n', 'r', 'r', 'v', 'v', 'a', 'a', 'n', 'n', 'r', 'r', 'v', 'v', 'r', 'a', 'n', 'r', 'a', 'r', 'r', 'n', 'n', 'r', 'a', 'n', 'v', 'n', 'r']\n"
     ]
    }
   ],
   "source": [
    "# Transform the tags into wordnet tags\n",
    "wordnet_tags = [get_wordnet_pos(p[1]) for p in posTokens]\n",
    "print(wordnet_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'September', '5th', ',', 'we', 'start', 'our', 'A.I', '.', 'course', '.', 'The', 'course', 'number', 'be', 'CSI4106', '.', 'In', 'that', 'course', ',', 'we', 'have', 'study', 'many', 'artificial', 'intelligence', 'concept', '.', 'We', 'have', 'look', 'at', 'intricate', 'algorithm', ',', 'such', 'as', 'the', 'ARC-3', 'algorithm', 'and', 'neural', 'network', 'learn', 'algorithm', '.']\n"
     ]
    }
   ],
   "source": [
    "# Now, let's try to lemmatize again, but we tell the lemmatizer what the POS is.\n",
    "\n",
    "posLemmas = [wnl.lemmatize(t,w) for t,w in zip(tokens,wordnet_tags)]\n",
    "print(posLemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO : Q2)** Which words are lemmatized differently when provided with the additional knowledge of POS? Look at the variables *lemmas* and *posLemmas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5   started   start\n",
      "14   is   be\n",
      "23   studied   study\n",
      "31   looked   look\n",
      "37   a   as\n",
      "44   learning   learn\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(posLemmas)):\n",
    "    if lemmas[i] != posLemmas[i]:\n",
    "        print(i, ' ', lemmas[i], ' ', posLemmas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q2-ANSWER:* From the code above, the changed words are: started, is, studied, looked, a, learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On September 5th, we started our A.I.', 'course.', 'The course number is CSI4106.', 'In that course, we have studied many artificial intelligence concepts.', 'We have looked at intricate algorithms, such as the ARC-3 algorithm and neural networks learning algorithms.']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Sentence splitting can be done before tokenizing and POS tagging if we wish\n",
    "\n",
    "sentences = nltk.sent_tokenize(sampleText)\n",
    "print(sentences)\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO - Q3)** How many sentences are generated?  If you try to \"reverse engineer\"... what could be the cause of the additional split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q3-ANSWER:* There are 5 sentences generated. The additional split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO - Q4)** Create another sample of text containing a few sentences.  You can copy sentences from a book or webpage, or just make them up.  Then, copy the pipeline steps we performed above to analyze your sentences.  Your pipeline should include: tokenization, POS tagging, POS-based lemmatization, and sentence splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 ANSWER\n",
    "\n",
    "# Rerun the pipeline on YOUR text\n",
    "# mySampleText = \"......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 - Parsing\n",
    "The parser in NLTK actually is a wrapper around a java parser (from Stanford NLP).  It's a bit complex to install, so instead, we will use the online Stanford parse to test a few sentences: http://nlp.stanford.edu:8080/parser/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO : Q5)** Run YOUR sentences in the parser.  Show the results of the dependency and constituency parsing (do copy/paste of portion of screen here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q5-ANSWER:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART B - STATISTICAL ANALYSIS OF TEXTS**\n",
    "\n",
    "We now look at a few statistics that help understand what a text is about and how it is stuctured. A quick \"summary\" of a text is obtained by looking at its important words.  Importance can be approximated by frequency.  \n",
    "\n",
    "Below we work with a small text copied from the page on Vitamin C in Wikipedia.  \n",
    "\n",
    "NLTK has many built-in methods to explore text. It's good to find some cheat sheet to see the set of methods.  [Here](https://github.com/michellejm/GCDRB_Text_Analysis/blob/master/Text-Analysis-with-NLTK-Cheatsheet.pdf) is one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitaminText =   \"Vitamin C, also known as ascorbic acid and L-ascorbic acid, is a vitamin found in food\"\\\n",
    "                \"and used as a dietary supplement.[1] The disease scurvy is prevented and treated with \"\\\n",
    "                \"vitamin C-containing foods or dietary supplements.[1] Evidence does not support use in \"\\\n",
    "                \"the general population for the prevention of the common cold.[2][3] There is, however, \"\\\n",
    "                \"some evidence that regular use may shorten the length of colds.[4] It is unclear if \"\\\n",
    "                \"supplementation affects the risk of cancer, cardiovascular disease, or dementia.[5][6] \"\\\n",
    "                \"It may be taken by mouth or by injection. Vitamin C is generally well tolerated.[1] Large doses \"\\\n",
    "                \"may cause gastrointestinal discomfort, headache, trouble sleeping, and flushing of the skin.[1][3] \"\\\n",
    "                \"Normal doses are safe during pregnancy.[7] The United States Institute of Medicine recommends \"\\\n",
    "                \"against taking large doses.[8] Vitamin C is an essential nutrient involved in the repair of \"\\\n",
    "                \"tissue and the enzymatic production of certain neurotransmitters.[1][8] It is required for the \"\\\n",
    "                \"functioning of several enzymes and is important for immune system function.[8][9] It also functions \"\\\n",
    "                \"as an antioxidant.[2] Foods containing vitamin C include citrus fruits, broccoli, Brussels sprouts, \"\\\n",
    "                \"raw bell peppers, and strawberries.[2] Prolonged storage or cooking may reduce vitamin C content \"\\\n",
    "                \" in foods.[2] Vitamin C was discovered in 1912, isolated in 1928, and in 1933 was the first vitamin \"\\\n",
    "                \"to be chemically produced.[10] It is on the World Health Organization Model List of Essential \"\\\n",
    "                \"Medicines, the most effective and safe medicines needed in a health system.[11] Vitamin C is available \"\\\n",
    "                \"as a generic medication and over-the-counter drug.[1] In 2015, the wholesale cost in the developing \"\\\n",
    "                \"world was less than US$0.01 per tablet.[12] Partly for its discovery, Albert Szent-Györgyi and \"\\\n",
    "                \"Walter Norman Haworth were awarded 1937 Nobel Prizes in Physiology and Medicine and Chemistry, \"\\\n",
    "                \"respectively.[13][14]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Gather the tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vitamin', 'c', ',', 'also', 'known', 'as', 'ascorbic', 'acid', 'and', 'l-ascorbic', 'acid', ',', 'is', 'a', 'vitamin', 'found', 'in', 'foodand', 'used', 'as', 'a', 'dietary', 'supplement', '.', '[', '1', ']', 'the', 'disease', 'scurvy', 'is', 'prevented', 'and', 'treated', 'with', 'vitamin', 'c-containing', 'foods', 'or', 'dietary', 'supplements', '.', '[', '1', ']', 'evidence', 'does', 'not', 'support', 'use', 'in', 'the', 'general', 'population', 'for', 'the', 'prevention', 'of', 'the', 'common', 'cold', '.', '[', '2', ']', '[', '3', ']', 'there', 'is', ',', 'however', ',', 'some', 'evidence', 'that', 'regular', 'use', 'may', 'shorten', 'the', 'length', 'of', 'colds', '.', '[', '4', ']', 'it', 'is', 'unclear', 'if', 'supplementation', 'affects', 'the', 'risk', 'of', 'cancer', ',', 'cardiovascular', 'disease', ',', 'or', 'dementia', '.', '[', '5', ']', '[', '6', ']', 'it', 'may', 'be', 'taken', 'by', 'mouth', 'or', 'by', 'injection', '.', 'vitamin', 'c', 'is', 'generally', 'well', 'tolerated', '.', '[', '1', ']', 'large', 'doses', 'may', 'cause', 'gastrointestinal', 'discomfort', ',', 'headache', ',', 'trouble', 'sleeping', ',', 'and', 'flushing', 'of', 'the', 'skin', '.', '[', '1', ']', '[', '3', ']', 'normal', 'doses', 'are', 'safe', 'during', 'pregnancy', '.', '[', '7', ']', 'the', 'united', 'states', 'institute', 'of', 'medicine', 'recommends', 'against', 'taking', 'large', 'doses', '.', '[', '8', ']', 'vitamin', 'c', 'is', 'an', 'essential', 'nutrient', 'involved', 'in', 'the', 'repair', 'of', 'tissue', 'and', 'the', 'enzymatic', 'production', 'of', 'certain', 'neurotransmitters', '.', '[', '1', ']', '[', '8', ']', 'it', 'is', 'required', 'for', 'the', 'functioning', 'of', 'several', 'enzymes', 'and', 'is', 'important', 'for', 'immune', 'system', 'function', '.', '[', '8', ']', '[', '9', ']', 'it', 'also', 'functions', 'as', 'an', 'antioxidant', '.', '[', '2', ']', 'foods', 'containing', 'vitamin', 'c', 'include', 'citrus', 'fruits', ',', 'broccoli', ',', 'brussels', 'sprouts', ',', 'raw', 'bell', 'peppers', ',', 'and', 'strawberries', '.', '[', '2', ']', 'prolonged', 'storage', 'or', 'cooking', 'may', 'reduce', 'vitamin', 'c', 'content', 'in', 'foods', '.', '[', '2', ']', 'vitamin', 'c', 'was', 'discovered', 'in', '1912', ',', 'isolated', 'in', '1928', ',', 'and', 'in', '1933', 'was', 'the', 'first', 'vitamin', 'to', 'be', 'chemically', 'produced', '.', '[', '10', ']', 'it', 'is', 'on', 'the', 'world', 'health', 'organization', 'model', 'list', 'of', 'essential', 'medicines', ',', 'the', 'most', 'effective', 'and', 'safe', 'medicines', 'needed', 'in', 'a', 'health', 'system', '.', '[', '11', ']', 'vitamin', 'c', 'is', 'available', 'as', 'a', 'generic', 'medication', 'and', 'over-the-counter', 'drug', '.', '[', '1', ']', 'in', '2015', ',', 'the', 'wholesale', 'cost', 'in', 'the', 'developing', 'world', 'was', 'less', 'than', 'us', '$', '0.01', 'per', 'tablet', '.', '[', '12', ']', 'partly', 'for', 'its', 'discovery', ',', 'albert', 'szent-györgyi', 'and', 'walter', 'norman', 'haworth', 'were', 'awarded', '1937', 'nobel', 'prizes', 'in', 'physiology', 'and', 'medicine', 'and', 'chemistry', ',', 'respectively', '.', '[', '13', ']', '[', '14', ']']\n"
     ]
    }
   ],
   "source": [
    "# Number of tokens\n",
    "vTokens = word_tokenize(vitaminText.lower())\n",
    "print(vTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides a class Text, which is instantiated using tokens. The class Text provides methods to easily build a frequency distribution of tokens, which shows the content of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', 25),\n",
       " (']', 25),\n",
       " ('.', 20),\n",
       " (',', 19),\n",
       " ('the', 16),\n",
       " ('and', 12),\n",
       " ('in', 11),\n",
       " ('vitamin', 10),\n",
       " ('is', 10),\n",
       " ('of', 9),\n",
       " ('c', 7),\n",
       " ('1', 6),\n",
       " ('it', 5),\n",
       " ('as', 4),\n",
       " ('a', 4),\n",
       " ('or', 4),\n",
       " ('for', 4),\n",
       " ('2', 4),\n",
       " ('may', 4),\n",
       " ('foods', 3),\n",
       " ('doses', 3),\n",
       " ('8', 3),\n",
       " ('was', 3),\n",
       " ('also', 2),\n",
       " ('acid', 2),\n",
       " ('dietary', 2),\n",
       " ('disease', 2),\n",
       " ('evidence', 2),\n",
       " ('use', 2),\n",
       " ('3', 2),\n",
       " ('be', 2),\n",
       " ('by', 2),\n",
       " ('large', 2),\n",
       " ('safe', 2),\n",
       " ('medicine', 2),\n",
       " ('an', 2),\n",
       " ('essential', 2),\n",
       " ('system', 2),\n",
       " ('world', 2),\n",
       " ('health', 2),\n",
       " ('medicines', 2),\n",
       " ('known', 1),\n",
       " ('ascorbic', 1),\n",
       " ('l-ascorbic', 1),\n",
       " ('found', 1),\n",
       " ('foodand', 1),\n",
       " ('used', 1),\n",
       " ('supplement', 1),\n",
       " ('scurvy', 1),\n",
       " ('prevented', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrap the tokens within a \"Text\" object on which we can apply methods.\n",
    "vText = nltk.Text(vTokens)\n",
    "# Number or tokens\n",
    "len(vText)\n",
    "# Build frequency distribution \n",
    "fdist = nltk.FreqDist(vText)\n",
    "fdist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many tokens shown above are not helpful in determining what the text is about.  We can use regular expressions to limit the desired tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the module for regular expressions\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 16),\n",
       " ('and', 12),\n",
       " ('in', 11),\n",
       " ('vitamin', 10),\n",
       " ('is', 10),\n",
       " ('of', 9),\n",
       " ('c', 7),\n",
       " ('it', 5),\n",
       " ('as', 4),\n",
       " ('a', 4),\n",
       " ('or', 4),\n",
       " ('for', 4),\n",
       " ('may', 4),\n",
       " ('foods', 3),\n",
       " ('doses', 3),\n",
       " ('was', 3),\n",
       " ('also', 2),\n",
       " ('acid', 2),\n",
       " ('dietary', 2),\n",
       " ('disease', 2),\n",
       " ('evidence', 2),\n",
       " ('use', 2),\n",
       " ('be', 2),\n",
       " ('by', 2),\n",
       " ('large', 2),\n",
       " ('safe', 2),\n",
       " ('medicine', 2),\n",
       " ('an', 2),\n",
       " ('essential', 2),\n",
       " ('system', 2),\n",
       " ('world', 2),\n",
       " ('health', 2),\n",
       " ('medicines', 2),\n",
       " ('known', 1),\n",
       " ('ascorbic', 1),\n",
       " ('found', 1),\n",
       " ('foodand', 1),\n",
       " ('used', 1),\n",
       " ('supplement', 1),\n",
       " ('scurvy', 1),\n",
       " ('prevented', 1),\n",
       " ('treated', 1),\n",
       " ('with', 1),\n",
       " ('supplements', 1),\n",
       " ('does', 1),\n",
       " ('not', 1),\n",
       " ('support', 1),\n",
       " ('general', 1),\n",
       " ('population', 1),\n",
       " ('prevention', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this regular expression says that a word must start and end with any number of letters.\n",
    "alphaTokens = [t for t in vTokens if re.match(\"^[a-zA-Z]+$\",t)]\n",
    "\n",
    "# generate another text, only with the alpha tokens\n",
    "alphaText = nltk.Text(alphaTokens)\n",
    "# build the frequency distribution again\n",
    "fdistAlpha = nltk.FreqDist(alphaText)\n",
    "fdistAlpha.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO : Q6)** We've removed the non alphanumeric characters.  Now continue the code below to remove both alphanumeric and *stopwords* from the set of tokens.  I let you first explore what *stopwords* are by decommenting the code below. And then, continue the code to show the top 50 tokens, once the filtering is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "{'be', 'its', \"weren't\", 'nor', 'her', 're', \"it's\", \"wasn't\", 'itself', \"hasn't\", 'mustn', 'the', 'can', 'as', 'weren', 'wouldn', 'll', 'about', 'does', 'while', 'so', 'but', 'we', 'against', 'i', 'to', 'any', 'been', \"couldn't\", \"hadn't\", 'should', \"doesn't\", 'haven', 'myself', 'hers', 'herself', 'them', 'than', 'he', 'my', 'a', 'such', \"that'll\", \"you've\", 'other', \"mightn't\", 'when', 'shan', \"you're\", 'through', 'don', 'in', 'shouldn', 'hasn', 'of', 'having', \"mustn't\", 'himself', 'yours', 'mightn', 'how', 'doing', 'until', 'below', 'by', 'these', 'from', 'some', \"didn't\", \"you'll\", 'up', 'is', 'were', \"wouldn't\", 'who', 'they', 'ours', 'isn', 'this', 'whom', 'his', 'further', 'didn', 'themselves', 'did', 'which', \"haven't\", 'why', 'because', 'hadn', \"aren't\", 'over', 'no', 'd', 'it', 'once', 'yourself', 'there', \"shouldn't\", 'with', 'each', \"isn't\", 'you', 'few', \"shan't\", 'off', 'out', \"should've\", 'those', 'will', 'only', 'very', 'that', 'just', 'before', 'if', 'has', 'and', 'y', 'ma', 'same', 'what', 'theirs', 'into', 'between', 'where', 'on', 'above', 'doesn', 'more', 'o', 'ain', 'am', 'your', \"you'd\", 'm', 'won', 'their', 'under', 'was', 'aren', 'not', 'an', \"won't\", 'ourselves', 'are', 'after', 'both', 'again', 'needn', 'had', \"don't\", 'couldn', 'yourselves', 've', 'most', 'for', 'him', 'or', 'now', 's', \"she's\", 'too', 'at', 'during', \"needn't\", 'me', 'wasn', 'down', 'own', 'then', 'do', 'she', 'all', 't', 'here', 'our', 'being', 'have'}\n"
     ]
    }
   ],
   "source": [
    "# Q6 - ANSWER\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# import the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "setStopWords = set(stopwords.words('english'))\n",
    "print(setStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(True, 295), (False, 104)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6 - ANSWER (continue)\n",
    "\n",
    "# build a filtered list of tokens\n",
    "filtered_tokens = [i not in stopwords.words('english') for i in vTokens]\n",
    "# we can reconstruct a Text object using the filtered list\n",
    "filteredText = nltk.Text(filtered_tokens)\n",
    "\n",
    "# build the frequency distribution again\n",
    "fdistFiltered = nltk.FreqDist(filteredText)\n",
    "# show top 50\n",
    "fdistFiltered.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO : Q7)** Download a page and gather statistics about that page content.  Write the code to show the top 50 tokens, after **lemmatizing, removing non-alphnumeric characters and removing stop words**.  You have all the elements of coding in the previous questions, but you need to put them together here. You can use the *Beethoven* page I used, or change it to a page of your choice.  I'm using a package called *BeautifulSoup* to help pre-process html pages and get to the raw text.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will download a page from Wikipedia \n",
    "\n",
    "from urllib import request\n",
    "url = \"https://en.wikipedia.org/wiki/Ludwig_van_Beethoven\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "# print(raw[1000:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from http://www.crummy.com/software/BeautifulSoup/\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "cleanRaw = BeautifulSoup(raw).get_text()\n",
    "print(cleanRaw[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 - ANSWER\n",
    "# Continue here... \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature\n",
    "I, -------YOUR NAME--------------, declare that the answers provided in this notebook are my own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
